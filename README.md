# Emotion Recognition in Customer Calls

> **Real-time audio â†’ speaker diarization â†’ ASR â†’ RCNN-Attention emotion tagging**  
> Macro-F1 0.83 Â· Real-Time Factor 0.12 on a GTX 1650 Â· End-to-end in < 30 s for a 2-min call.

---

## âœ¨ Key Features
| Stage | Open-Source Backbone | Latency (2-min call) | Notes |
|-------|----------------------|----------------------|-------|
| VAD & Segmentation | WebRTC VAD | 0.26 s | configurable padding |
| Speaker Diarization | `pyannote/speaker-diarization` | 16.8 s | DER â‰ˆ 11 % |
| ASR | **faster-whisper Large-v3** | 9.9 s | WER â‰ˆ 9.4 % |
| Emotion Classifier | **RCNN-Attention** (1.5 M params) | 1.2 s | 7-class |

* Lightweight: 4 GB GPU RAM fits full pipeline  
* Streaming-friendly: emits segment emotions every 1 s  
* React dashboard (optional) for live heat-maps

---

## ðŸ—ï¸ Repository Structure
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ asr_diarize.py # VAD + diarizer + Whisper wrapper
â”‚ â”œâ”€â”€ hf_emotion.py # RCNN-Attn inference
â”‚ â”œâ”€â”€ pipeline_emotion.py # orchestration script
â”‚ â”œâ”€â”€ rcnn_attention.py # model definition
â”‚ â””â”€â”€ log.py # unified logger
â”œâ”€â”€ models/ # checkpoints (or HF IDs)
â”œâ”€â”€ data/ # example wav & transcripts
â”œâ”€â”€ docker/ # Dockerfile & helm chart
â””â”€â”€ README.md # you are here


---

## ðŸš€ Quick Start (GPU)

```bash
git clone https://github.com/your-org/emo-pipeline.git
cd emo-pipeline
pip install -r requirements.txt          # Python 3.11 + PyTorch >= 2.5

# Download IEMOCAP sample (already included in data/) and run:
python src/pipeline_emotion.py \
    data/sample_call.wav \
    --hf-token <your_HF_token_for_pyannote> \
    --device cuda

âœ“ diarized transcript â†’ sample_call.dia.json   (segments=44)
âœ“ emotions â†’ sample_call.dia_emo.json
âœ“ report saved to sample_call_report.txt
